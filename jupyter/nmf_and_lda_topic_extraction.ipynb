{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF and LDA Topic Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 200\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.874s.\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = dataset.data[:n_samples]\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won't be bummin' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \\n--\\nBake Timmons, III\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_samples[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples = [\n",
    "        \"cat and dogs are animals\",\n",
    "        \"i love animals but not cats\",\n",
    "        \"steve jobs was the founder of apple\",\n",
    "        \"jobs made back to the apple after getting fired\",\n",
    "        \"car has brakes and gear\",\n",
    "        \"brakes of the car failed\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.066s.\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in 0.065s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=200 and n_features=1000...\n",
      "done in 0.082s.\n",
      "\n",
      "Topics in NMF model (Frobenius norm):\n",
      "Topic #0: don just like know people good say time does make\n",
      "Topic #1: vram simms simm board need favorite buy 30 cost correct\n",
      "Topic #2: key chip keys clipper session receiving door knows needs government\n",
      "Topic #3: info drivers mail help anybody monitor does windows hi tell\n",
      "Topic #4: memory goes story person 49 free data real probably digital\n",
      "Topic #5: bike riding know rights rear motorcycle people luck following mean\n",
      "Topic #6: game heard flyers sure regular season remember red comes mean\n",
      "Topic #7: win means favorite event baseball bet text runs read does\n",
      "Topic #8: captain traded mike currently pittsburgh season course real toronto time\n",
      "Topic #9: think pontiac pretty european sure assembly switch signal easy just\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '040', '10', '100', '1000', '11', '12', '128', '13', '14', '15', '16', '17', '18', '19', '1993', '20', '200', '21', '22', '23', '24', '25', '250', '26', '27', '28', '30', '32', '32k', '33', '386bsd', '3d', '40', '42', '49', '50', '500', '604', '66', '72', '75', '80', '90', 'abc', 'able', 'ac', 'academic', 'accept', 'access', 'act', 'actually', 'add', 'addition', 'additional', 'address', 'administration', 'ago', 'agree', 'algorithm', 'allowed', 'americans', 'analysis', 'animation', 'announced', 'annual', 'answer', 'answers', 'anybody', 'appeared', 'appears', 'apple', 'application', 'applications', 'appropriate', 'april', 'area', 'article', 'aside', 'ask', 'asked', 'asking', 'assault', 'assembly', 'assume', 'attack', 'attacks', 'attempt', 'au', 'author', 'automatic', 'available', 'average', 'away', 'bad', 'baltimore', 'ban', 'bank', 'base', 'baseball', 'based', 'basically', 'begin', 'believe', 'best', 'bet', 'better', 'bible', 'big', 'bigger', 'bike', 'bikes', 'bit', 'bitnet', 'bits', 'blocks', 'blood', 'board', 'bob', 'bodies', 'body', 'book', 'boston', 'bother', 'bought', 'box', 'break', 'brown', 'buffer', 'build', 'building', 'built', 'bunch', 'business', 'buy', 'buying', 'ca', 'cable', 'cache', 'california', 'called', 'came', 'canada', 'captain', 'car', 'card', 'cards', 'care', 'carrying', 'case', 'cause', 'caused', 'cd', 'cellular', 'center', 'certain', 'certainly', 'change', 'changed', 'channel', 'cheap', 'cheaper', 'check', 'cheers', 'children', 'chip', 'chips', 'choice', 'choose', 'christ', 'circle', 'cities', 'citizens', 'city', 'claim', 'clinton', 'clipper', 'close', 'coast', 'code', 'collection', 'color', 'colorado', 'com', 'come', 'comes', 'comfortable', 'coming', 'command', 'commercial', 'common', 'community', 'companies', 'company', 'compared', 'compiler', 'complete', 'computer', 'computers', 'concept', 'condition', 'conference', 'connect', 'connection', 'connector', 'consider', 'contact', 'containing', 'contains', 'contents', 'continue', 'control', 'conversion', 'convert', 'converter', 'copy', 'correct', 'cost', 'costs', 'couldn', 'couple', 'course', 'create', 'created', 'creation', 'criminals', 'cross', 'cup', 'current', 'currently', 'data', 'database', 'date', 'david', 'day', 'days', 'dc', 'dead', 'deal', 'death', 'dec', 'decided', 'defend', 'defense', 'defensive', 'define', 'defined', 'definition', 'degree', 'deleted', 'deny', 'department', 'des', 'described', 'description', 'design', 'desktop', 'device', 'devices', 'did', 'didn', 'difference', 'different', 'digital', 'disagree', 'discussion', 'disk', 'display', 'distributed', 'distribution', 'doctrine', 'does', 'doesn', 'doing', 'domain', 'don', 'door', 'dos', 'dr', 'draw', 'drive', 'driver', 'drivers', 'drives', 'drop', 'earth', 'easier', 'easily', 'easy', 'ed', 'edu', 'effect', 'effective', 'effort', 'el', 'electronic', 'email', 'encrypted', 'end', 'enforcement', 'entire', 'entry', 'eric', 'error', 'errors', 'especially', 'europe', 'european', 'event', 'evidence', 'ex', 'exactly', 'example', 'excellent', 'exchange', 'executables', 'exist', 'expect', 'experience', 'extensions', 'extra', 'eye', 'fact', 'factor', 'failure', 'faith', 'fancy', 'faq', 'far', 'fast', 'favorite', 'fax', 'features', 'feed', 'feel', 'figure', 'file', 'files', 'final', 'finally', 'fine', 'fit', 'florida', 'flyers', 'folks', 'follow', 'following', 'font', 'force', 'forget', 'format', 'formats', 'fpu', 'fred', 'free', 'french', 'friend', 'fully', 'future', 'game', 'games', 'gateway', 'gave', 'general', 'george', 'getting', 'given', 'gl', 'gm', 'god', 'goes', 'going', 'gold', 'good', 'got', 'gotten', 'gov', 'government', 'grade', 'graphics', 'great', 'green', 'greg', 'ground', 'group', 'gs', 'guess', 'guide', 'gun', 'guy', 'half', 'ham', 'hand', 'happen', 'happened', 'happy', 'hard', 'hardware', 'hate', 'having', 'hawks', 'hd', 'head', 'hear', 'heard', 'heart', 'heat', 'height', 'hell', 'help', 'helps', 'henry', 'hi', 'high', 'higher', 'hit', 'hits', 'hockey', 'hold', 'hole', 'home', 'hope', 'hours', 'house', 'hp', 'huge', 'human', 'id', 'idea', 'ii', 'il', 'image', 'imagine', 'importance', 'important', 'include', 'included', 'includes', 'including', 'increase', 'info', 'information', 'inside', 'installed', 'instructions', 'insurance', 'interested', 'interesting', 'interface', 'interfaces', 'internal', 'internet', 'intersection', 'irit', 'islanders', 'isn', 'israel', 'israeli', 'issue', 'jesus', 'jews', 'jim', 'jobs', 'john', 'june', 'just', 'key', 'keyboard', 'keys', 'kids', 'killed', 'killing', 'kind', 'knew', 'know', 'known', 'knows', 'lab', 'land', 'language', 'large', 'later', 'latest', 'law', 'leafs', 'lebanese', 'left', 'legitimate', 'let', 'letter', 'lev', 'level', 'library', 'license', 'lies', 'life', 'light', 'like', 'likely', 'line', 'lines', 'linux', 'list', 'lists', 'little', 'live', 'lives', 'll', 'load', 'local', 'logical', 'long', 'look', 'looking', 'looks', 'lot', 'lots', 'love', 'low', 'luck', 'lunar', 'mac', 'machine', 'machines', 'macintosh', 'mail', 'main', 'major', 'make', 'makes', 'making', 'male', 'man', 'manager', 'manual', 'manuals', 'mark', 'martin', 'mary', 'material', 'materials', 'math', 'matter', 'maybe', 'mean', 'means', 'media', 'meg', 'members', 'memory', 'men', 'message', 'metal', 'methods', 'mi', 'mike', 'miles', 'milwaukee', 'mind', 'minority', 'minutes', 'mirrors', 'missing', 'mistake', 'mit', 'mm', 'model', 'modeling', 'models', 'modem', 'money', 'monitor', 'months', 'moon', 'moral', 'motif', 'motorcycle', 'mouse', 'mp', 'mr', 'ms', 'mu', 'music', 'named', 'nasa', 'national', 'nature', 'near', 'necessary', 'need', 'needed', 'needs', 'net', 'network', 'new', 'news', 'nhl', 'nice', 'non', 'normal', 'north', 'northern', 'note', 'notes', 'nsfnet', 'number', 'numbers', 'object', 'objects', 'obvious', 'obviously', 'offer', 'oh', 'ok', 'okay', 'old', 'operations', 'opinions', 'option', 'options', 'order', 'organization', 'original', 'oz', 'pa', 'package', 'packages', 'paper', 'papers', 'parallel', 'particular', 'parts', 'party', 'patch', 'patches', 'paul', 'pay', 'pc', 'pd', 'peace', 'people', 'performance', 'person', 'personal', 'peter', 'phigs', 'phone', 'phones', 'pick', 'picture', 'pictures', 'piece', 'pitt', 'pittsburgh', 'place', 'places', 'plan', 'plane', 'planned', 'platform', 'play', 'plus', 'point', 'points', 'police', 'policy', 'political', 'pontiac', 'poor', 'port', 'portable', 'ported', 'possibility', 'possible', 'post', 'posted', 'posting', 'postscript', 'pov', 'power', 'pre', 'present', 'president', 'press', 'pretty', 'price', 'print', 'printer', 'probably', 'problem', 'problems', 'processing', 'processor', 'product', 'program', 'programming', 'programs', 'project', 'promote', 'proof', 'properly', 'property', 'public', 'purchase', 'purchased', 'purposes', 'putting', 'question', 'questions', 'quickly', 'quite', 'radio', 'ram', 'ran', 'random', 'range', 'rangers', 'read', 'reading', 'real', 'reality', 'realize', 'really', 'rear', 'reason', 'reasons', 'receive', 'received', 'receiving', 'recently', 'record', 'red', 'references', 'regards', 'registered', 'regular', 'release', 'relevant', 'remember', 'removed', 'rendering', 'rent', 'repeat', 'reply', 'report', 'reports', 'request', 'requests', 'require', 'research', 'resolution', 'resources', 'response', 'responsible', 'result', 'return', 'rid', 'ride', 'riding', 'right', 'rights', 'rl', 'road', 'robert', 'rom', 'round', 'rpi', 'rt', 'rule', 'rules', 'run', 'running', 'runs', 'safety', 'said', 'sale', 'salt', 'samples', 'save', 'say', 'saying', 'says', 'school', 'sci', 'science', 'scope', 'screen', 'se', 'search', 'season', 'second', 'secret', 'sectors', 'secure', 'seen', 'self', 'semi', 'senate', 'send', 'sending', 'sense', 'sent', 'series', 'service', 'services', 'session', 'set', 'shareware', 'short', 'shouldn', 'shuttle', 'signal', 'similar', 'simm', 'simms', 'simple', 'simply', 'single', 'site', 'situation', 'size', 'small', 'software', 'soldiers', 'solution', 'soon', 'sorry', 'sort', 'sounds', 'source', 'sources', 'south', 'space', 'specific', 'specifically', 'speed', 'sphere', 'st', 'standard', 'start', 'state', 'stay', 'steering', 'step', 'stereo', 'stop', 'storage', 'store', 'story', 'street', 'stress', 'strip', 'stuff', 'subject', 'sun', 'support', 'sure', 'surface', 'surfaces', 'suspect', 'suspension', 'swing', 'switch', 'switzerland', 'systems', 'table', 'taking', 'talk', 'talking', 'tape', 'tar', 'tcl', 'team', 'tech', 'technical', 'technology', 'tell', 'telling', 'term', 'terms', 'terrorist', 'terrorists', 'test', 'testament', 'text', 'thanks', 'theory', 'thesis', 'thing', 'things', 'think', 'thinking', 'thomas', 'thought', 'thread', 'ti', 'time', 'times', 'today', 'took', 'toolkit', 'tools', 'topic', 'toronto', 'total', 'trace', 'trade', 'traded', 'trial', 'tried', 'trip', 'troops', 'trouble', 'true', 'try', 'trying', 'turbo', 'turkish', 'turn', 'turned', 'tv', 'type', 'types', 'uiuc', 'uk', 'understand', 'understanding', 'unfortunately', 'university', 'unix', 'unless', 'updates', 'usa', 'use', 'used', 'usenet', 'user', 'users', 'uses', 'using', 'usually', 'utilities', 'value', 'various', 've', 'vehicle', 'version', 'vga', 'video', 'villages', 'visit', 'visualization', 'voice', 'volume', 'vote', 'voted', 'vp', 'vr', 'vram', 'vs', 'wait', 'want', 'wanted', 'washington', 'water', 'waterloo', 'way', 'weapons', 'week', 'went', 'wheels', 'white', 'widgets', 'width', 'willing', 'win', 'windows', 'wish', 'won', 'wondering', 'word', 'words', 'work', 'worked', 'working', 'works', 'world', 'worth', 'wouldn', 'write', 'writing', 'written', 'wrong', 'x11', 'x11r4', 'year', 'years', 'yes', 'york', 'young']\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf features, n_samples=200 and n_features=1000...\n",
      "done in 0.288s.\n",
      "\n",
      "Topics in NMF model (generalized Kullback-Leibler divergence):\n",
      "Topic #0: good use like just years things don ve make thing\n",
      "Topic #1: work using edu program systems lines use running speed school\n",
      "Topic #2: government people doesn rights right long non weapons public going\n",
      "Topic #3: know does thanks don says say called mean change people\n",
      "Topic #4: really year memory vram need card runs season don ll\n",
      "Topic #5: pretty look think just want don know problem friend work\n",
      "Topic #6: said new believe time info windows heard write drive leafs\n",
      "Topic #7: like just usually used doing thanks problem running trying width\n",
      "Topic #8: time new trip traded season mike good use way just\n",
      "Topic #9: think subject group 11 try start understanding read power fact\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
    "      \"tf-idf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features, n_samples=200 and n_features=1000...\n",
      "done in 0.452s.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: insurance know good drive car does don riding bike cable\n",
      "Topic #1: try current problem sense range keyboard hand thought university suspension\n",
      "Topic #2: win like way really better year don contact going think\n",
      "Topic #3: think don like just good people power use time coming\n",
      "Topic #4: program drive errors hard read cache went keyboard power gs\n",
      "Topic #5: edu graphics mail send 128 3d com file format objects\n",
      "Topic #6: just captain traded use new hp blood believe rules going\n",
      "Topic #7: don like just key chip know plane years problem used\n",
      "Topic #8: israel israeli people time lebanese peace soldiers men accept members\n",
      "Topic #9: gm game cache john time st vs baltimore card ram\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: bike riding know mistake luck chips rights rear mean reading\n",
      "Topic #1: power time speed think opinions members just say thought colorado\n",
      "Topic #2: drive radio problems money vram pc end cache phigs gs\n",
      "Topic #3: don did law piece edu right 200 self evidence called\n",
      "Topic #4: mail program ago errors does cities baseball monitor help discussion\n",
      "Topic #5: book think problem certain faith make really just actually white\n",
      "Topic #6: just traded going captain mike peter long got posting season\n",
      "Topic #7: chip effective bob community like processor heat channel problem normal\n",
      "Topic #8: fpu israeli israel usually makes jim solution ban turn seen\n",
      "Topic #9: game looking memory mind insurance packages boston time help week\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda.fit(tfidf)\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
      "             evaluate_every=-1, learning_decay=0.7,\n",
      "             learning_method='online', learning_offset=50.0,\n",
      "             max_doc_update_iter=100, max_iter=5, mean_change_tol=0.001,\n",
      "             n_components=10, n_jobs=1, n_topics=None, perp_tol=0.1,\n",
      "             random_state=0, topic_word_prior=None,\n",
      "             total_samples=1000000.0, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "print(lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
